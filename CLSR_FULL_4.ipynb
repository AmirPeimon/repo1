{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Estimating \"PURPOSE\" of Bridges by \"Classification\"\n",
    "\n",
    "Raw Data Resource:\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/bridges/bridges.data.version1\n",
    "\n",
    "##### Dataframe is composed of \n",
    "- three scalar features:\n",
    "   1. LOCATION\n",
    "   2. ERECTED \n",
    "   3. LENGTH\n",
    "   \n",
    "   \n",
    "- nine categorical features:\n",
    "   4. RIVER\n",
    "   5. PURPOSE\n",
    "   6. LANES\n",
    "   7. CLEAR-G\n",
    "   8. T-OR-D\n",
    "   9. MATERIAL\n",
    "   10. SPAN\n",
    "   11. REL-L\n",
    "   12. TYPE \n",
    "   \n",
    "    \n",
    "##### Purpose of Project:\n",
    "Making an Estimator to predict the value of feature 5:  **\"PURPOSE\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "\n",
    "import statistics as stat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.utils import resample\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading data \n",
    "\n",
    "df_raw = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00537/sobar-72.csv', header=0) # delimiter=';'\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing columns header\n",
    "\n",
    "\n",
    "df_raw.columns = ['sexualRisk', 'eating', 'Hygine',\n",
    "       'aggregation', 'commitment', 'consistency',\n",
    "       'spontaneity', 'Person', 'fulfill',\n",
    "       'vulnerability', 'severity',\n",
    "       'strength', 'willingness',\n",
    "       'emotionality', 'appreciation',\n",
    "       'instrumental', 'knowledge',\n",
    "       'abilities', 'desires', 'cervix']\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# deleting unnecessary columns (eg. ID)\n",
    "\n",
    "del_columns = [ 'sexualRisk' ] \n",
    "\n",
    "keep_columns = []\n",
    "for c in df_raw.columns:\n",
    "    if c not in del_columns:\n",
    "        keep_columns.append(c)\n",
    "    \n",
    "df_raw = df_raw.loc[ : , keep_columns ]\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindexing\n",
    "df_raw = df_raw.reindex(range(0,df_raw.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Columns\n",
    "##### columns of dtype='object'  may contain:\n",
    " - 'whitespaces' in strings  \n",
    " - 'missing-values' which are usually shown by question-mark, '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function called  find_obj_cols  which takes a dataframe\n",
    "# and returns list of columns with  dtype=='object'\n",
    "\n",
    "def find_obj_cols(df):\n",
    "    cols = df.columns[ df.dtypes=='object' ]\n",
    "    return cols\n",
    "\n",
    "# object columns \n",
    "obj_cols = find_obj_cols( df_raw ); print('obj_cols',obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help\n",
    "obj_cols_index = [ ]\n",
    "\n",
    "# (auto)\n",
    "for i in obj_cols_index:\n",
    "    c = obj_cols[i]; print(' ', c, ' ', sorted(df_raw.loc[:,c].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (input) white_cols  &  qmark_cols  &  str_cols\n",
    "white_cols = [ ]; print('white_cols',white_cols)\n",
    "qmark_cols = [ ]; print('qmark_cols',qmark_cols)\n",
    "\n",
    "# (input) integers wrongfully as object  &  floats wrongfully as objects\n",
    "int_o   = [ ]; print('int_o',int_o)\n",
    "float_o = [ ]; print('float_o',float_o)\n",
    "\n",
    "# (auto) str_cols\n",
    "str_cols =[]\n",
    "for c in obj_cols:\n",
    "    if c not in int_o+float_o:\n",
    "        str_cols.append(c)\n",
    "print('str_cols',str_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (auto) define a function called  find_int_cols  which takes a dataframe\n",
    "# and returns list of columns with  dtype=='int64'\n",
    "def find_int_cols(df):\n",
    "    cols = df.columns[ df.dtypes=='int64' ]\n",
    "    return cols\n",
    "\n",
    "# (auto) integer-columns mixed with float-columns \n",
    "int_i   = find_int_cols( df_raw );  print('int_i',int_i)\n",
    "float_i = []; print('float_i',float_i)  # always empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (auto) define a function called  find_float_cols  which takes a dataframe\n",
    "# and returns list of columns with  dtype=='float64'\n",
    "def find_float_cols(df):\n",
    "    cols = df.columns[ df.dtypes=='float64' ]\n",
    "    return cols\n",
    "\n",
    "# (auto) float-columns mixed with integer-columns\n",
    "float_cols_dirty = find_float_cols( df_raw ); print('float_cols_dirty',float_cols_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help\n",
    "float_cols_dirty_index = list( np.arange(7,9) )\n",
    "\n",
    "# (auto)\n",
    "for i in float_cols_dirty_index:\n",
    "    c = float_cols_dirty[i]; print(' ', c, ' ', np.round( sorted(df_raw.loc[:,c].unique()),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'float_cols_dirty' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f7eb5646ca96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfloat_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfloat_cols_dirty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mint_f\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mfloat_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'float_cols_dirty' is not defined"
     ]
    }
   ],
   "source": [
    "# (input) index of int-columns wrongfully in float_cols_dirty\n",
    "int_index = [  ]\n",
    "\n",
    "# (auto)\n",
    "int_f = []\n",
    "for i in int_index:\n",
    "    c = float_cols_dirty[i]\n",
    "    int_f.append(c)\n",
    "     \n",
    "float_f = []\n",
    "for c in float_cols_dirty:\n",
    "    if c not in int_f:\n",
    "        float_f.append( c )\n",
    "        \n",
    "print('int_f',int_f); print('\\nfloat_f',float_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_cols & float_cols (auto)\n",
    "\n",
    "int_cols   = list(int_o)   + list(int_i)   + list(int_f);   print('int_cols',int_cols)\n",
    "float_cols = list(float_o) + list(float_i) + list(float_f); print('\\nfloat_cols',float_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing *'whitespaces'* in *dtype='object'* columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (auto)\n",
    "df_dirty = df_raw.copy()\n",
    "\n",
    "# define a function called fix_whitespaces\n",
    "# to remove whitespaces from begining/end of strings\n",
    "\n",
    "def fix_whitespaces( df, cols ):\n",
    "    n = df.shape[0]\n",
    "    for c in cols:\n",
    "        s=[]\n",
    "        for i in range(0,n):\n",
    "            s.append( df.loc[i,c].strip() )\n",
    "        df.loc[:,c] = s\n",
    "    return df\n",
    "\n",
    "# removing\n",
    "df_no_white = fix_whitespaces( df_dirty, white_cols )\n",
    "    \n",
    "# Checking  \n",
    "for c in obj_cols:\n",
    "    print('\\n', c, '\\n', sorted(df_no_white.loc[:,c].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing '?' in *dtype='object'* columns\n",
    " - in numeric columns,  '?' => \"median\" of numbers\n",
    " - in string columns, '?' => \"mode\" of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (auto)\n",
    "df_dirty = df_no_white.copy() \n",
    " \n",
    "# define a function called fix_qmarks\n",
    "# which returns corrected df[ft]\n",
    "\n",
    "def fix_qmarks( df, ft, str_cols, num_cols ):\n",
    "    \n",
    "    x = df[ft].copy()\n",
    "    n = df.shape[0]\n",
    "    \n",
    "    if ft in str_cols:   \n",
    "        typ = 'str' \n",
    "    else:  \n",
    "        typ = 'num' \n",
    "               \n",
    "    val=[]\n",
    "    for r in range(0,n):\n",
    "        if (  (x[r])=='?'  ):\n",
    "            ;\n",
    "        else:\n",
    "            val.append( x[r] )\n",
    "    \n",
    "    if (  typ=='str'  ):\n",
    "        val = np.array(val).astype('str')\n",
    "        ave = stat.mode(val)  \n",
    "    elif (  typ=='num'  ):\n",
    "        val = np.array(val).astype('float64')\n",
    "        ave = stat.median(val)\n",
    "    else:\n",
    "        print(\"typ is neither  'str'  nor  'int'  nor  'float'\")\n",
    "        return df[ft]\n",
    "    \n",
    "    for r in range(0,n):\n",
    "        if (  x[r]=='?'  ):\n",
    "            x[r]=ave\n",
    "    \n",
    "    print(ft,typ)\n",
    "    \n",
    "    if   (  typ=='str'  ):\n",
    "        return x.astype('str') \n",
    "    else:\n",
    "        return x.astype('float64')\n",
    "\n",
    "# fixing columns\n",
    "for ft in qmark_cols: \n",
    "    df_dirty.loc[:,ft] = fix_qmarks( df_dirty, ft, str_cols, int_cols+float_cols )\n",
    "     \n",
    "\n",
    "df_no_qmark = df_dirty.copy()\n",
    "print( df_no_qmark.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing *'blank cells'* in all columns (auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding blank-cols\n",
    "df_dirty = df_no_qmark.copy() \n",
    "\n",
    "# Boolean function\n",
    "# True: feature has blank_cells\n",
    "def has_blank(df,ft): \n",
    "    x = df[ft].count()\n",
    "    n = df.shape[0] \n",
    "    return x<n\n",
    "\n",
    "# blank_cols\n",
    "blank_cols=[]\n",
    "for c in df_dirty.columns:\n",
    "    if has_blank( df_dirty, c ):\n",
    "        blank_cols.append( c ) \n",
    "\n",
    "print('blank_cols',blank_cols)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing blank-cols\n",
    "\n",
    "# define a function called fix_blank\n",
    "# which returns corrected df[ft]\n",
    "\n",
    "def fix_blank( df, ft, str_cols, int_cols, float_cols ):\n",
    "    \n",
    "    x = df[ft].copy()\n",
    "    n = df.shape[0]\n",
    "    \n",
    "    if ft in str_cols:   \n",
    "        typ='str'\n",
    "    elif ft in int_cols: \n",
    "        typ = 'int'\n",
    "    elif ft in float_cols: \n",
    "        typ = 'float'\n",
    "    else:   \n",
    "        print('\\nWarning!\\n ', ft, ' is neither in  str_cols  nor in  int_cols  nor in  float_cols')\n",
    "        return x\n",
    "        \n",
    "    val=[]\n",
    "    for r in range(0,n):\n",
    "        if (  pd.notnull(x[r])==True  ): \n",
    "            val.append( x[r] )\n",
    "    \n",
    "    if (  typ=='str'  ):\n",
    "        val = np.array(val).astype('str')\n",
    "        ave = stat.mode(val) \n",
    "    elif (  typ=='int'  ):\n",
    "        val = np.array(val).astype('int')\n",
    "        ave = stat.median(val)\n",
    "    elif (  typ=='float'  ):\n",
    "        val = np.array(val).astype('float')\n",
    "        ave = stat.median(val)\n",
    "    else:\n",
    "        print(\"typ is neither  'str'  nor  'int'  nor  'float'\")\n",
    "        return df[ft]\n",
    "    \n",
    "    for r in range(0,n):\n",
    "        if (  pd.isnull(x[r])==True  ):  \n",
    "            x[r]=ave\n",
    "    \n",
    "    if   (  typ=='str'  ):  \n",
    "        return x.astype('str')\n",
    "    elif (  typ=='int'  ):  \n",
    "        return x.astype('int64')                 \n",
    "    else:\n",
    "        return x.astype('float64')\n",
    "\n",
    "# fixing columns\n",
    "for ft in blank_cols: \n",
    "    df_dirty.loc[:,ft]=fix_blank( df_dirty, ft, str_cols, int_cols, float_cols )\n",
    "\n",
    "df_no_blank = df_dirty.copy()\n",
    "df_no_blank.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_no_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_missing\n",
    "df_no_missing = df_no_blank.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   \n",
    "### fixing dtype of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_no_missing.copy()\n",
    "\n",
    "for c in int_cols:   df[c]=df[c].astype('int64')\n",
    "for c in float_cols: df[c]=df[c].astype('float64')\n",
    "for c in str_cols:   df[c]=df[c].astype('str')\n",
    "    \n",
    "df_no_missing = df.copy()\n",
    "\n",
    "print('int_cols',int_cols); print('\\nfloat_cols',float_cols); print('\\nstr_cols',str_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing columns into 'Scalar' and 'Categorical'\n",
    "The ordinal columns with less than 5 unique values are 'Categorical'. \n",
    "The ordinal columns with 5 and more unique values are 'Scalar'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guide\n",
    "'''                                         *\n",
    "   dtype            n_uniques               *\n",
    "                n<=4         n>=5           *              ordinal     non-ordinal\n",
    "                                            *                       \n",
    "   object    categorical   categorical      *       XXX:   scalar      categorical\n",
    "   float       scalar        scalar         *\n",
    "    int      categorical      XXX           *\n",
    "                                            *    \n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc & cat\n",
    "df = df_no_missing\n",
    "\n",
    "cat, sc, xxx = [], [], []\n",
    "for c in df.columns: \n",
    "    if   df[c].dtype=='object' : cat.append(c)\n",
    "    elif df[c].dtype=='float64': sc.append(c)\n",
    "    elif len(df[c].unique())<=4: cat.append(c)\n",
    "    else                       : xxx.append(c)  \n",
    "        \n",
    "print('cat',cat); print('\\nsc',sc); print('\\nxxx',xxx)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help\n",
    "indexes = [0,1,2] \n",
    "for i in indexes:\n",
    "    c = xxx[i] \n",
    "    print( ' ', i,'  #', len(df_no_missing[c].unique()), ' ', sorted(df_no_missing[c].unique()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_non_ordinal_index = [ ]\n",
    "\n",
    "# (auto)\n",
    "non_ordinal_columns,  ordinal_columns  =  [], []\n",
    "for i in range(0,len(xxx)):\n",
    "    if i in xxx_non_ordinal_index: non_ordinal_columns.append( xxx[i] )\n",
    "    else:                              ordinal_columns.append( xxx[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (auto)\n",
    "scalar_columns     = list(sc)  + list(ordinal_columns);     print(' scalar_columns',scalar_columns)\n",
    "categorical_columns= list(cat) + list(non_ordinal_columns); print('\\ncategorical_columns',categorical_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting order of columns:  scalar first, categorical second  \n",
    "df_no_missing = df_no_missing.loc[ :, scalar_columns+categorical_columns ]\n",
    "\n",
    "for c in categorical_columns:\n",
    "    df_no_missing.loc[:,c] = df_no_missing.loc[:,c].astype(str) \n",
    "    \n",
    "df_no_missing.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Typically there is some noise (dirt) in the data which should be cleaned.\n",
    "\n",
    "##### There are two approaches to clean the data:\n",
    "1. Statistical Methods, including:\n",
    " - Inter Quartile Range (IQR): to detect general univariate outliers\n",
    " - z-values: to detect extreme univariate outliers\n",
    " - mahalanobis distance test: to detect multivariate outliers\n",
    "\n",
    "\n",
    "2. Machine-Learning Methods, including:\n",
    " - Isolation Forest \n",
    " - Eliptic_envelope\n",
    " - Local Outlier Factor\n",
    " - One Class SVM \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                                         #\n",
    "#                  Visualizing Outliers                   #\n",
    "#                        by Boxplot                       # \n",
    "#                                                         #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function called \"plot_boxplot\"\n",
    "\n",
    "def plot_boxplot(df,ft):   \n",
    "    df.boxplot(column=[ft])\n",
    "    plt.grid(False)\n",
    "    plt.show()    \n",
    "    \n",
    "for c in scalar_columns:\n",
    "    plot_boxplot( df_no_missing, c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help\n",
    "print( scalar_columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining columns with outliers\n",
    "outlier_cols = [ 'eating',   'consistency', 'spontaneity', 'strength' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                                         #\n",
    "#                Fixing \"Extreme\" Outliers                # \n",
    "#                     using \"z-values\"                    #\n",
    "#                                                         #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option (1) Winsorizing Extreme Outliers\n",
    "\n",
    "# df_dirty\n",
    "df_dirty = df_no_missing.copy()\n",
    "\n",
    "# define a function called winsorize\n",
    "# which converts extreme outliers to threshhold\n",
    "# and returns list of winsorized indexes\n",
    "# z = (x-M) / SD\n",
    "# +/- 3    \n",
    "\n",
    "def winsorize(df,ft):\n",
    "    x  = df[ft].copy()\n",
    "    M  = x.mean()\n",
    "    SD = x.std()\n",
    "    z  = (x-M) / SD\n",
    "    \n",
    "    upper_bound = +3\n",
    "    lower_bound = -3\n",
    "    \n",
    "    ls = df.index[  (z > upper_bound) \n",
    "                  | (z < lower_bound) ]\n",
    "    \n",
    "    x_Des=sorted(x,reverse=True)\n",
    "    i=0\n",
    "    while x_Des[i] >=  M + 3*SD:\n",
    "        i=i+1 \n",
    "    MAX=x_Des[i] \n",
    "    \n",
    "    x_Asc=sorted(x,reverse=False)\n",
    "    i=0 \n",
    "    while x_Asc[i] <=  M - 3*SD:\n",
    "        i=i+1  \n",
    "    MIN=x_Asc[i] \n",
    "    \n",
    "    for i in range(0,len(x)):\n",
    "        if   z[i] > upper_bound: x[i]=MAX\n",
    "        elif z[i] < lower_bound: x[i]=MIN\n",
    "            \n",
    "    return [x, ls]\n",
    "\n",
    "\n",
    "# create a function to store the output indices \n",
    "# from multiple columns\n",
    "\n",
    "df_no_extreme = df_dirty.copy()\n",
    "\n",
    "index_list = []\n",
    "for c in outlier_cols:\n",
    "    c_clean, ls = winsorize( df_dirty, c )\n",
    "    index_list.extend( ls )\n",
    "    df_no_extreme.loc[ :, c ] = c_clean\n",
    "\n",
    "df_extreme = df_no_missing.iloc[ sorted(index_list), : ] \n",
    "print( len(df_no_extreme)/len(df_raw) *100 )\n",
    "df_no_extreme.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option (2) Removing Extreme Outliers\n",
    "'''\n",
    "# df_dirty\n",
    "df_dirty = df_no_missing.copy()\n",
    "\n",
    "# define a function called outliers\n",
    "# which returns a list of index of outliers\n",
    "# z = (x-M) / SD\n",
    "# +/- 3    \n",
    "\n",
    "def outliers(df,ft):\n",
    "    x  = df[ft]\n",
    "    M  = x.mean()\n",
    "    SD = x.std()\n",
    "    z  = (x-M) / SD\n",
    "    \n",
    "    upper_bound = +3\n",
    "    lower_bound = -3\n",
    "    \n",
    "    ls = df.index[  (z > upper_bound) \n",
    "                  | (z < lower_bound) ]\n",
    "    \n",
    "    return ls\n",
    "\n",
    "\n",
    "\n",
    "# create a function to store the output indices \n",
    "# from multiple columns    \n",
    "    \n",
    "index_list = []\n",
    "for feature in outlier_cols:  \n",
    "    index_list.extend( outliers(df_dirty,feature) )\n",
    "\n",
    "# define a function called \"remove_extreme_outliers_by_zValues\"\n",
    "# which returns a dataframe without extreme outliers \n",
    "\n",
    "def remove_extreme_outliers_by_zValues(df, ls):\n",
    "    ls = sorted(set(ls))\n",
    "    df = df.drop(ls)\n",
    "    return df \n",
    "\n",
    "df_extreme    = df_dirty.iloc[ sorted(index_list), : ]\n",
    "df_no_extreme = remove_extreme_outliers_by_zValues( df_dirty, index_list )\n",
    "len(df_no_extreme)/len(df_raw) *100\n",
    "\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing extreme outliers\n",
    "df_dirty = pd.concat([df_no_extreme[scalar_columns],df_extreme[scalar_columns]],axis=0)\n",
    "\n",
    "flag_clean   = np.ones( df_no_extreme.shape[0] ) * (+1)\n",
    "flag_outlier = np.ones( df_extreme.shape[0]    ) * (-1) \n",
    "\n",
    "flag = list(flag_clean)\n",
    "for i in list(flag_outlier):\n",
    "    flag.append(i)\n",
    "\n",
    "# PCA\n",
    "x = df_dirty \n",
    "y = []\n",
    "for i in range(0,len(flag)):\n",
    "    if flag[i]==1: y.append('cyan')\n",
    "    else:          y.append('red')\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit_transform( x )\n",
    " \n",
    "x_pca = scale( pca.fit_transform( x ) )\n",
    "x_pc1 = x_pca[:,0] \n",
    "x_pc2 = x_pca[:,1] \n",
    "\n",
    "# PCA Scatter Plot \n",
    "fig, ax = plt.subplots( figsize=(10,6) )  \n",
    "\n",
    "scatter = ax.scatter( \n",
    "      x_pc1\n",
    "    , x_pc2 \n",
    "    #, cmap = 'rainbow'\n",
    "    , c    = y\n",
    "    , s    = 300\n",
    "    , edgecolors = 'k'\n",
    "    , alpha      = 0.55 \n",
    "    )\n",
    " \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA Plot\\nClean-Data  VS  Extreme-Outliers') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                                         #\n",
    "#            Removing \"Multivariate\" Outliers             # \n",
    "#              using \"Mahalanobis Distance\"               #\n",
    "#                                                         #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dirty\n",
    "#df_dirty = df_no_missing.copy()\n",
    "df_dirty = df_no_extreme.copy()\n",
    "\n",
    "# define a function called \"MahalanobisDist\"\n",
    "# which returns the Mahalanobis Distance (MD) for each record\n",
    "\n",
    "def MahalanobisDist(df, verbose=False):\n",
    "    covariance_matrix = np.cov(df, rowvar=False)\n",
    "    if is_pos_def(covariance_matrix):\n",
    "        inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "        if is_pos_def(inv_covariance_matrix):\n",
    "            vars_mean = []\n",
    "            for i in range(df.shape[0]):\n",
    "                vars_mean.append(list(df.mean(axis=0)))\n",
    "            diff = df - vars_mean\n",
    "            md = []\n",
    "            for i in range(len(diff)):\n",
    "                md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Covariance Matrix:\\n {}\\n\".format(covariance_matrix))\n",
    "                print(\"Inverse of Covariance Matrix:\\n {}\\n\".format(inv_covariance_matrix))\n",
    "                print(\"Variables Mean Vector:\\n {}\\n\".format(vars_mean))\n",
    "                print(\"Variables - Variables Mean Vector:\\n {}\\n\".format(diff))\n",
    "                print(\"Mahalanobis Distance:\\n {}\\n\".format(md))\n",
    "            return md\n",
    "        else:\n",
    "            print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n",
    "    else:\n",
    "        print(\"Error: Covariance Matrix is not positive definite!\")\n",
    "\n",
    "def is_pos_def(A):\n",
    "    if np.allclose(A, A.T):\n",
    "        try:\n",
    "            np.linalg.cholesky(A)\n",
    "            return True\n",
    "        except np.linalg.LinAlgError:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# finding mahalanobis distance for each record\n",
    "md = MahalanobisDist(df_dirty[scalar_columns].to_numpy(), verbose=0)\n",
    "\n",
    "\n",
    "# defining a function called \"multivar_outliers\" \n",
    "# which returns list of indexes of multivariate outliers \n",
    "\n",
    "def multivar_outliers(df,md):\n",
    "    #df['MD'] = md\n",
    "    M  = np.mean(md)\n",
    "    SD = np.std(md)\n",
    "    threshhold = M + 3.0*SD\n",
    "    ls = df.index[  (md > threshhold)  ]\n",
    "    return ls\n",
    " \n",
    "index_list = multivar_outliers( df_dirty[scalar_columns], md )\n",
    "\n",
    "\n",
    "# define a function called \"remove\"\n",
    "# which removes multivariate outliers\n",
    "\n",
    "def remove_multivar_outliers(df, ls):\n",
    "    ls = sorted(set(ls))\n",
    "    df = df.drop(ls)\n",
    "    return df \n",
    "\n",
    "df_no_multivar = remove_multivar_outliers( df_dirty, index_list ) \n",
    "df_multivar    = df_dirty.drop( df_no_multivar.index ) \n",
    "len(df_no_multivar)/len(df_raw) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing multivariate outliers\n",
    "df_dirty = pd.concat([df_no_multivar[scalar_columns],df_multivar[scalar_columns]],axis=0)\n",
    "\n",
    "flag_clean   = np.ones( df_no_multivar.shape[0] ) * (+1)\n",
    "flag_outlier = np.ones( df_multivar.shape[0]    ) * (-1) \n",
    "\n",
    "flag = list(flag_clean)\n",
    "for i in list(flag_outlier):\n",
    "    flag.append(i)\n",
    "\n",
    "# PCA\n",
    "x = df_dirty \n",
    "y = []\n",
    "for i in range(0,len(flag)):\n",
    "    if flag[i]==1: y.append('cyan')\n",
    "    else:          y.append('red')\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit_transform( x )\n",
    " \n",
    "x_pca = scale( pca.fit_transform( x ) )\n",
    "x_pc1 = x_pca[:,0] \n",
    "x_pc2 = x_pca[:,1] \n",
    "\n",
    "# PCA Scatter Plot \n",
    "# Clean VS Multivar\n",
    "fig, ax = plt.subplots( figsize=(10,6) )  \n",
    "\n",
    "scatter = ax.scatter( \n",
    "      x_pc1\n",
    "    , x_pc2 \n",
    "    #, cmap = 'rainbow'\n",
    "    , c    = y\n",
    "    , s    = 300\n",
    "    , edgecolors = 'k'\n",
    "    , alpha      = 0.55 \n",
    "    )\n",
    " \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA Plot\\nClean-Data  VS  Multivariate-Outliers') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                                         #\n",
    "#       Cleaning scalar data using Isolation-Forest       # \n",
    "#                                                         #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dirty = df_no_missing.copy()\n",
    "#df_dirty = df_no_extreme.copy()\n",
    "df_dirty = df_no_multivar.copy()\n",
    "\n",
    "\n",
    "# define a function called Clean_by_IForest\n",
    "# to clean 'scalar columns' using 'Isolation Forest'\n",
    "\n",
    "def remove_outliers_by_IForest( df, sc_cols, c ):\n",
    "    sc_data = df[ sc_cols ].values\n",
    "    IForest = IsolationForest( contamination=c ) \n",
    "    IForest.fit( sc_data )\n",
    "    flag_clean = IForest.predict( sc_data ) == +1\n",
    "    return df.loc[ flag_clean, : ]\n",
    "\n",
    "df_no_noise = remove_outliers_by_IForest( df_dirty, scalar_columns, 0.01 )   # 4% contamination considered\n",
    "df_noise = df_dirty.drop( df_no_noise.index )   \n",
    "\n",
    "len(df_no_noise)/len(df_raw) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing noise\n",
    "df_dirty = pd.concat([df_no_noise[scalar_columns],df_noise[scalar_columns]],axis=0)\n",
    "\n",
    "flag_clean   = np.ones( df_no_noise.shape[0] ) * (+1)\n",
    "flag_outlier = np.ones( df_noise.shape[0] ) * (-1) \n",
    "\n",
    "flag = list(flag_clean)\n",
    "for i in list(flag_outlier):\n",
    "    flag.append(i)\n",
    "\n",
    "# PCA  \n",
    "x = df_dirty \n",
    "y = []\n",
    "for i in range(0,len(flag)):\n",
    "    if flag[i]==1: y.append('cyan')\n",
    "    else:          y.append('red')\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit_transform( x )\n",
    " \n",
    "x_pca = scale( pca.fit_transform( x ) )\n",
    "x_pc1 = x_pca[:,0] \n",
    "x_pc2 = x_pca[:,1] \n",
    "\n",
    "# PCA Scatter Plot \n",
    "# Clean VS Multivar\n",
    "fig, ax = plt.subplots( figsize=(10,6) )  \n",
    "\n",
    "scatter = ax.scatter( \n",
    "      x_pc1\n",
    "    , x_pc2 \n",
    "    #, cmap = 'rainbow'\n",
    "    , c    = y\n",
    "    , s    = 300\n",
    "    , edgecolors = 'k'\n",
    "    , alpha      = 0.55 \n",
    "    )\n",
    " \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA Plot\\nClean-Data  VS  Noise') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking Cleaned data at each stage\n",
    " \n",
    "# Outliers    \n",
    "Outlier_Dict = { \n",
    "      'Method'       : [  'z-values'\n",
    "                        , 'Mahalanobis_Distance'\n",
    "                        , 'Isolation Forest']\n",
    "    \n",
    "    , 'Outlier_Type' : [  'Extreme' \n",
    "                        , 'Multivariate'        \n",
    "                        , 'Noise'           ]\n",
    "    \n",
    "    , 'Clean'        : [  len(df_no_extreme)\n",
    "                        , len(df_no_multivar)\n",
    "                        , len(df_no_noise) ]\n",
    "    \n",
    "    , 'Outlier'      : [  len(df_extreme)\n",
    "                        , len(df_multivar)\n",
    "                        , len(df_noise) ]\n",
    "    \n",
    "    , 'Clean/No_Missing (%)' : [  len(df_no_extreme)/len(df_no_missing)  *100\n",
    "                                , len(df_no_multivar)/len(df_no_missing) *100\n",
    "                                , len(df_no_noise)/len(df_no_missing)    *100 ] }\n",
    "\n",
    "Outlier = pd.DataFrame( Outlier_Dict ) \n",
    "Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_no_noise.copy()\n",
    "df_clean.head( 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                                         #\n",
    "#             Inspecting the categorical data             # \n",
    "#                                                         #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function callled plot_bars\n",
    "# to show frequency of each item in each category\n",
    "\n",
    "def plot_bars(df, ft):\n",
    "    import random\n",
    "    \n",
    "    for c in ft:\n",
    "        # Prepare Data\n",
    "        S = df.groupby( c ).size().reset_index( name='counts' ) \n",
    "        S = S.sort_values('counts',ascending=False)\n",
    "        S[c]=S[c].astype('str')\n",
    "        n = S[c].unique().__len__()\n",
    "        all_colors = list( plt.cm.colors.cnames.keys() )\n",
    "        random.seed(1000)\n",
    "        colors = random.choices( all_colors, k=n )\n",
    "        \n",
    "        # Plot Bars\n",
    "        plt.figure(figsize=(10,2), dpi= 80)\n",
    "        plt.bar( S[c], S['counts'], color=colors, width=.5 )\n",
    "        for i, val in enumerate( S['counts'].values ):\n",
    "            plt.text(  i, val, float(val)\n",
    "                     , horizontalalignment='center', verticalalignment='bottom'\n",
    "                     , fontdict={'fontweight':500, 'size':16}\n",
    "                    )\n",
    " \n",
    "        # Decoration\n",
    "        #plt.gca().set_xticklabels( S[c], rotation=0, horizontalalignment='center', fontsize=16 )\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        plt.title( c, fontsize=18)\n",
    "        plt.ylabel( 'counts', fontsize=16 )\n",
    "        #plt.ylim(0, 40) \n",
    "        #plt.savefig('composition_04v_Bar_Chart.png') \n",
    "        plt.show()\n",
    "            \n",
    "plot_bars( df_clean, categorical_columns+ordinal_columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                                         #\n",
    "#                    Downsampling Data                    #\n",
    "#                                                         #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a column to use its sub-categories to downsample\n",
    "chosen_column = 'cervix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function callled count_cats\n",
    "# to show frequency of each item in each category\n",
    "\n",
    "def count_cats(df, ft):\n",
    "    for c in ft:\n",
    "        S = df[c]\n",
    "        print( '\\n', c, sorted(S.unique()), len(S) )\n",
    "        for x in sorted( S.unique() ):\n",
    "            y = S[S==x]\n",
    "            print( '  ', x, ' ', len(y)) \n",
    "            \n",
    "count_cats( df_clean, [chosen_column] )\n",
    "print('\\n\"'+chosen_column+'\" has',len(df_clean[chosen_column].unique()),'sub-cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using The Estimated Feature to Downsample\n",
    "\n",
    "c = df_clean[ chosen_column ]\n",
    "d = sorted( c.unique() )\n",
    "\n",
    "df_0 = df_clean[ c==d[0] ]  # 143 recordS\n",
    "df_1 = df_clean[ c==d[1] ]  # 123 recordS \n",
    "#df_2 = df_clean[ c==d[2] ]  # 146 recordS \n",
    "#df_3 = df_clean[ c==d[3] ]  # 131 recordS   \n",
    "\n",
    "print(  len(df_0)\n",
    "      , len(df_1)\n",
    "      #, len(df_2)\n",
    "      #, len(df_3)  \n",
    "     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down_sampling  (resize)\n",
    "df_0_downsampled = resample( df_0, replace=False, n_samples=len(df_0), random_state=0 )\n",
    "df_1_downsampled = resample( df_1, replace=False, n_samples=len(df_1), random_state=0 )\n",
    "#df_2_downsampled = resample( df_2, replace=False, n_samples=len(df_2), random_state=0 )\n",
    "#df_3_downsampled = resample( df_3, replace=False, n_samples=len(df_3), random_state=0 ) \n",
    "\n",
    "print(  len(df_0_downsampled)\n",
    "      , len(df_1_downsampled)\n",
    "      #, len(df_2_downsampled)\n",
    "      #, len(df_3_downsampled)  \n",
    "     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging down_sampled datasets\n",
    "df_sample = pd.concat( [  df_0_downsampled     \n",
    "                        , df_1_downsampled     \n",
    "                        #, df_2_downsampled     \n",
    "                        #, df_3_downsampled  \n",
    "                       ] )\n",
    "len( df_sample )  # 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                                         #\n",
    "#                   Formatting the Data                   #\n",
    "#                                                         #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1:  defining x & y\n",
    "\n",
    "x = df_sample.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: One-Hot Encoding of categorical data on x\n",
    "\n",
    "encode_columns = []\n",
    "for ft in categorical_columns:\n",
    "    encode_columns.append(ft)\n",
    "        \n",
    "x_encoded = pd.get_dummies( x, columns=encode_columns )\n",
    "x_encoded.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: training and testing sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: scaling x\n",
    "x_scaled = scale( x_encoded ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Clusters\n",
    "#####   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                                         #\n",
    "#              Building Preliminary Clusters              # \n",
    "#                                                         #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters \n",
    "nClsr = 3\n",
    "\n",
    "# clusterers                                   or a guess    defaults is 10\n",
    "kmean   =          KMeans( n_clusters=nClsr,  init='random',  n_init=200  )\n",
    "mbkmean = MiniBatchKMeans( n_clusters=nClsr,  init='random',  n_init=200  )\n",
    "\n",
    "clsr = kmean\n",
    "\n",
    "\n",
    "clsr.fit( x_scaled )  \n",
    "\n",
    "clsr_centroids = clsr.cluster_centers_\n",
    "clsr_ss        = silhouette_score( x_scaled, clsr.labels_ )  # the smaller the better\n",
    "clsr_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing\n",
    "y_predict = clsr.predict( x_scaled ) \n",
    "\n",
    "pca = PCA() \n",
    "\n",
    "x_pca        = pca.fit_transform( x_scaled )\n",
    "x_pca_scaled = scale( \n",
    "    np.column_stack(( \n",
    "          x_pca[:,0]\n",
    "        , x_pca[:,1] \n",
    "        )))\n",
    " \n",
    "# pc1 pc2\n",
    "x_pc1_scaled = x_pca_scaled[:,0] \n",
    "x_pc2_scaled = x_pca_scaled[:,1]  \n",
    "\n",
    "# PCA Scatter Plot \n",
    "fig, ax = plt.subplots( figsize=(10,6) )    \n",
    "\n",
    "scatter = ax.scatter( \n",
    "      x_pc1_scaled\n",
    "    , x_pc2_scaled \n",
    "    , cmap = 'rainbow_r'\n",
    "    , c    = y_predict\n",
    "    , s    = 300\n",
    "    , edgecolors = 'k'\n",
    "    , alpha      = 0.55 \n",
    "    )\n",
    " \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA Plot')\n",
    "\n",
    "legend = ax.legend( scatter.legend_elements()[0],\n",
    "                    scatter.legend_elements()[1],\n",
    "                    loc='best') \n",
    "\n",
    "\n",
    "### Plotting Center\n",
    "cc = clsr_centroids.copy()\n",
    "cc = np.concatenate( [cc,x_scaled], axis=0 ) \n",
    "cc = scale( cc ) \n",
    "cc_pca = pca.fit_transform( cc )\n",
    "\n",
    "cc_pca_scaled = scale( \n",
    "    np.column_stack(( \n",
    "          cc_pca[:,0]\n",
    "         ,cc_pca[:,1] \n",
    "        )))\n",
    "\n",
    "cc_pc1_scaled = cc_pca_scaled[range(0,nClsr),0] \n",
    "cc_pc2_scaled = cc_pca_scaled[range(0,nClsr),1] \n",
    "\n",
    "scatter = ax.scatter( \n",
    "      cc_pc1_scaled\n",
    "    , cc_pc2_scaled \n",
    "    , cmap = 'rainbow_r'\n",
    "    , c    = 'k'\n",
    "    , s    = 350\n",
    "    , edgecolors = 'k'\n",
    "    , alpha      = 1.00 \n",
    "    , label      = 'centeroid'\n",
    "    )   \n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                                         #\n",
    "#               Preparing to make predictions             #\n",
    "#                                                         #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceptable Range\n",
    "print('\\n Acceptable Range ... \\n')\n",
    "\n",
    "# Range of scalar_columns\n",
    "Range = pd.concat([df_clean[scalar_columns].min(),df_clean[scalar_columns].max()],axis=1)\n",
    "Range.columns = ['Min','Max']\n",
    "print( np.round( Range, 2 ), '\\n' )\n",
    "\n",
    "# Range of categorical_columns\n",
    "for col in categorical_columns:\n",
    "    items = sorted( df_clean[ col ].unique() )\n",
    "    print( col, '   ', items )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   \n",
    "## The Estimator\n",
    "##### Input data. Note the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted item\n",
    "df_no_missing.loc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data                #  Range   \n",
    "eating         =   8        #  8   15\n",
    "Hygine         =   3        #  3   15\n",
    "aggregation    =   5        #  2   10\n",
    "commitment     =  10        #  6   15\n",
    "consistency    =   5        #  4   10\n",
    "spontaneity    =   7        #  5   10\n",
    "Person         =   3        #  1    5\n",
    "fulfill        =   9        #  3   15\n",
    "vulnerability  =   9        #  3   15\n",
    "severity       =   6        #  2   10\n",
    "strength       =  10        #  4   15\n",
    "willingness    =   9        #  3   15\n",
    "emotionality   =   9        #  3   15\n",
    "appreciation   =   6        #  2   10\n",
    "instrumental   =   9        #  3   15\n",
    "knowledge      =   9        #  3   15\n",
    "abilities      =   9        #  3   15\n",
    "desires        =   9        #  3   15 \n",
    "\n",
    "cervix         =  '0'       # '0'  '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   \n",
    "## The Estimation\n",
    "#####  ( auto )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help\n",
    "x.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s  written in the same order as  x\n",
    "s = [  eating, Hygine, aggregation, commitment, consistency,\n",
    "       spontaneity, Person, fulfill, vulnerability, severity,\n",
    "       strength, willingness, emotionality, appreciation,\n",
    "       instrumental, knowledge, abilities, desires, cervix ]\n",
    " \n",
    "# define a function called predicted_class\n",
    "# which returns a string of the predicted class\n",
    "def predicted_class( s, x, encode_columns ):\n",
    "    ss = pd.DataFrame( [s.copy(),s.copy()], columns=x.columns )\n",
    "    sx = pd.concat( [ss,x], axis=0 )\n",
    "    sx_encoded = pd.get_dummies(\n",
    "          sx\n",
    "        , columns = encode_columns\n",
    "        )\n",
    "    sx_scaled = scale(  sx_encoded.iloc[1:,:]  )\n",
    "    sx_scaled = sx_scaled[[0,1]]\n",
    "    Estimation = clsr.predict( sx_scaled )[0] \n",
    "    \n",
    "    return Estimation    \n",
    "\n",
    "pred = predicted_class( s, x, categorical_columns )\n",
    "print('Estimation:  Cluster', pred ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   \n",
    "## The Visualization\n",
    "#####  ( auto )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing\n",
    "y_predict = clsr.predict( x_scaled ) \n",
    "\n",
    "pca = PCA() \n",
    "\n",
    "x_pca        = pca.fit_transform( x_scaled )\n",
    "x_pca_scaled = scale( \n",
    "    np.column_stack(( \n",
    "          x_pca[:,0]\n",
    "        , x_pca[:,1] \n",
    "        )))\n",
    " \n",
    "# pc1 pc2\n",
    "x_pc1_scaled = x_pca_scaled[:,0] \n",
    "x_pc2_scaled = x_pca_scaled[:,1]  \n",
    "\n",
    "# PCA Scatter Plot \n",
    "fig, ax = plt.subplots( figsize=(10,6) )    \n",
    "\n",
    "scatter = ax.scatter( \n",
    "      x_pc1_scaled\n",
    "    , x_pc2_scaled \n",
    "    , cmap = 'rainbow_r'\n",
    "    , c    = y_predict\n",
    "    , s    = 300\n",
    "    , edgecolors = 'k'\n",
    "    , alpha      = 0.55 \n",
    "    )\n",
    " \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA Plot\\nEstimation = '+np.str(pred)) \n",
    "\n",
    "legend = ax.legend( scatter.legend_elements()[0],\n",
    "                    scatter.legend_elements()[1],\n",
    "                    loc='best' ) \n",
    "\n",
    "\n",
    "### Plotting Estimation\n",
    "ss = pd.DataFrame( [s.copy(),s.copy()], columns=x.columns )\n",
    "sx = pd.concat( [ss,x], axis=0 )\n",
    "sx_encoded = pd.get_dummies(\n",
    "      sx\n",
    "    , columns = encode_columns\n",
    "    )\n",
    "sx_scaled = scale(  sx_encoded.iloc[1:,:]  )\n",
    "ss = sx_scaled[[0,0]]  \n",
    "ss = np.concatenate( [ss,x_scaled], axis=0 ) \n",
    "ss = scale( ss ) \n",
    "ss_pca = pca.fit_transform( ss )\n",
    "\n",
    "ss_pca_scaled = scale( \n",
    "    np.column_stack(( \n",
    "          ss_pca[:,0]\n",
    "         ,ss_pca[:,1] \n",
    "        )))\n",
    "\n",
    "ss_pc1_scaled = ss_pca_scaled[0,0] \n",
    "ss_pc2_scaled = ss_pca_scaled[0,1] \n",
    "\n",
    "scatter = ax.scatter( \n",
    "      ss_pc1_scaled\n",
    "    , ss_pc2_scaled \n",
    "    , cmap = 'rainbow_r'\n",
    "    , c    = 'white'\n",
    "    , s    = 500\n",
    "    , edgecolors = 'k' \n",
    "    , alpha      = 0.99  \n",
    "    )   \n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
